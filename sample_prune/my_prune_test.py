import torch
from torch import nn
import torch.nn.utils.prune as prune
import torch.nn.functional as F

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class LeNet(nn.Module):
    def __init__(self):
        super(LeNet, self).__init__()
        # 1 input image channel, 6 output channels, 3x3 square conv kernel
        self.conv1 = nn.Conv2d(1, 6, 3)
        self.conv2 = nn.Conv2d(6, 16, 3)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5x5 image dimension
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = x.view(-1, int(x.nelement() / x.shape[0]))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

model = LeNet().to(device=device)


######################################################################
# Inspect a Module
# ----------------
# 
# Let's inspect the (unpruned) ``conv1`` layer in our LeNet model. It will contain two 
# parameters ``weight`` and ``bias``, and no buffers, for now.
module = model.conv1
print('module.named_parameters')
print(list(module.named_parameters()))

######################################################################
print('module.named_buffers')
print(list(module.named_buffers()))

print('module.weight')
print(module.weight)

#A . do prune
print('don prone')
prune.random_unstructured(module, name="weight", amount=0.3) 

print('module.named_parameters')
print(list(module.named_parameters()))

######################################################################
# The pruning mask generated by the pruning technique selected above is saved 
# as a module buffer named ``weight_mask`` (i.e. appending ``"_mask"`` to the 
# initial parameter ``name``).
print('module.named_buffers()')
print(list(module.named_buffers()))

######################################################################
# For the forward pass to work without modification, the ``weight`` attribute 
# needs to exist. The pruning techniques implemented in 
# ``torch.nn.utils.prune`` compute the pruned version of the weight (by 
# combining the mask with the original parameter) and store them in the 
# attribute ``weight``. Note, this is no longer a parameter of the ``module``,
# it is now simply an attribute.
print('module.weight')
print(module.weight)

print('module._forward_pre_hooks')
print(module._forward_pre_hooks)


# B. prune bias
print('prune bias ...................................')
prune.l1_unstructured(module, name="bias", amount=3)

print('module.named_parameters()')
print(list(module.named_parameters()))

print('module.named_buffers()')
print(list(module.named_buffers()))

print('module.bias')
print(module.bias)

print('module._forward_pre_hooks')
print(module._forward_pre_hooks)

print('prune.ln_structured weight ...................................')
prune.ln_structured(module, name="weight", amount=0.5, n=2, dim=0)

print('module.weight')
print(module.weight)


for hook in module._forward_pre_hooks.values():
    print('hook:{}'.format(hook))
    print('hook._tensor_name:{}'.format(hook._tensor_name))
    if hook._tensor_name == "weight":  # select out the correct hook
        break

print('hook dump:')
print(list(hook))  # pruning history in the container 

print('model.state_dict().keys():')
print(model.state_dict().keys())

print('module.named_parameters():')
print(list(module.named_parameters()))

print('module.named_buffers():')
print(list(module.named_buffers()))

print('module.weight:')
print(module.weight)

print('remove weight........................')
prune.remove(module, 'weight')

print('module.named_parameters():')
print(list(module.named_parameters()))

print('module.named_buffers():')
print(list(module.named_buffers()))

print('prune conv2d and fc with different rate........................')
new_model = LeNet()
for name, module in new_model.named_modules():
    # prune 20% of connections in all 2D-conv layers 
    if isinstance(module, torch.nn.Conv2d):
        prune.l1_unstructured(module, name='weight', amount=0.2)
    # prune 40% of connections in all linear layers 
    elif isinstance(module, torch.nn.Linear):
        prune.l1_unstructured(module, name='weight', amount=0.4)

print('new_model.named_buffers()).keys():')
print(dict(new_model.named_buffers()).keys())  # to verify that all masks exist


print('global prune.............................................')
model = LeNet()

parameters_to_prune = (
    (model.conv1, 'weight'),
    (model.conv2, 'weight'),
    (model.fc1, 'weight'),
    (model.fc2, 'weight'),
    (model.fc3, 'weight'),
)

prune.global_unstructured(
    parameters_to_prune,
    pruning_method=prune.L1Unstructured,
    amount=0.2,
)

######################################################################
# Now we can check the sparsity induced in every pruned parameter, which will 
# not be equal to 20% in each layer. However, the global sparsity will be 
# (approximately) 20%.
print(
    "Sparsity in conv1.weight: {:.2f}%".format(
        100. * float(torch.sum(model.conv1.weight == 0))
        / float(model.conv1.weight.nelement())
    )
)
print(
    "Sparsity in conv2.weight: {:.2f}%".format(
        100. * float(torch.sum(model.conv2.weight == 0))
        / float(model.conv2.weight.nelement())
    )
)
print(
    "Sparsity in fc1.weight: {:.2f}%".format(
        100. * float(torch.sum(model.fc1.weight == 0))
        / float(model.fc1.weight.nelement())
    )
)
print(
    "Sparsity in fc2.weight: {:.2f}%".format(
        100. * float(torch.sum(model.fc2.weight == 0))
        / float(model.fc2.weight.nelement())
    )
)
print(
    "Sparsity in fc3.weight: {:.2f}%".format(
        100. * float(torch.sum(model.fc3.weight == 0))
        / float(model.fc3.weight.nelement())
    )
)
print(
    "Global sparsity: {:.2f}%".format(
        100. * float(
            torch.sum(model.conv1.weight == 0)
            + torch.sum(model.conv2.weight == 0)
            + torch.sum(model.fc1.weight == 0)
            + torch.sum(model.fc2.weight == 0)
            + torch.sum(model.fc3.weight == 0)
        )
        / float(
            model.conv1.weight.nelement()
            + model.conv2.weight.nelement()
            + model.fc1.weight.nelement()
            + model.fc2.weight.nelement()
            + model.fc3.weight.nelement()
        )
    )
)


class FooBarPruningMethod(prune.BasePruningMethod):
    """Prune every other entry in a tensor
    """
    PRUNING_TYPE = 'unstructured'

    def compute_mask(self, t, default_mask):
        mask = default_mask.clone()
        mask.view(-1)[::2] = 0 
        return mask

######################################################################
# Now, to apply this to a parameter in an ``nn.Module``, you should
# also provide a simple function that instantiates the method and
# applies it.
def foobar_unstructured(module, name):
    """Prunes tensor corresponding to parameter called `name` in `module`
    by removing every other entry in the tensors.
    Modifies module in place (and also return the modified module) 
    by:
    1) adding a named buffer called `name+'_mask'` corresponding to the 
    binary mask applied to the parameter `name` by the pruning method.
    The parameter `name` is replaced by its pruned version, while the 
    original (unpruned) parameter is stored in a new parameter named 
    `name+'_orig'`.

    Args:
        module (nn.Module): module containing the tensor to prune
        name (string): parameter name within `module` on which pruning
                will act.

    Returns:
        module (nn.Module): modified (i.e. pruned) version of the input
            module
    
    Examples:
        >>> m = nn.Linear(3, 4)
        >>> foobar_unstructured(m, name='bias')
    """
    FooBarPruningMethod.apply(module, name)
    return module

######################################################################
# Let's try it out!
model = LeNet()
foobar_unstructured(model.fc3, name='bias')

print('model.fc3.bias_mask........')
print(model.fc3.bias_mask)